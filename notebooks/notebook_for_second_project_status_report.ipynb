{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "IAsagUsbjuFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "hujEozS3xfcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "FdysHSjW9hrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "590kJkxjBGv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's download dataset"
      ],
      "metadata": {
        "id": "MkEw-6f-BHq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDb Movie Reviews: A dataset containing movie reviews labeled as positive or negative sentiment. It's commonly used for sentiment analysis tasks."
      ],
      "metadata": {
        "id": "ri83wuTiBhgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "texts = imdb_dataset[\"train\"][\"text\"]\n",
        "labels = imdb_dataset[\"train\"][\"label\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n"
      ],
      "metadata": {
        "id": "LDeDkSCKBwKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels, label_counts = np.unique(y_train, return_counts=True)\n",
        "\n",
        "# Print the label frequencies\n",
        "for label, count in zip(unique_labels, label_counts):\n",
        "    print(f\"Label {label}: {count} samples\")"
      ],
      "metadata": {
        "id": "r7nlHEkaSQ3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels, label_counts = np.unique(y_val, return_counts=True)\n",
        "\n",
        "# Print the label frequencies\n",
        "for label, count in zip(unique_labels, label_counts):\n",
        "    print(f\"Label {label}: {count} samples\")"
      ],
      "metadata": {
        "id": "8knYVYiLS6BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[10]"
      ],
      "metadata": {
        "id": "OKnrEQxOETHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained En-BERT-base model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "jYHD7msVG-GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_length = 128  # Maximum sequence length for BERT\n",
        "train_token_ids = [tokenizer.encode(text, max_length=max_length, truncation=True) for text in X_train]\n",
        "val_token_ids = [tokenizer.encode(text, max_length=max_length, truncation=True) for text in X_val]\n",
        "\n",
        "\n",
        "train_token_ids = [ids + [0] * (max_length - len(ids)) for ids in train_token_ids]\n",
        "val_token_ids = [ids + [0] * (max_length - len(ids)) for ids in val_token_ids]\n",
        "\n",
        "train_token_ids_tensor = torch.tensor(train_token_ids)\n",
        "val_token_ids_tensor = torch.tensor(val_token_ids)\n",
        "train_labels_tensor = torch.tensor(y_train)\n",
        "val_labels_tensor = torch.tensor(y_val)\n",
        "\n",
        "train_dataset = TensorDataset(train_token_ids_tensor, train_labels_tensor)\n",
        "val_dataset = TensorDataset(val_token_ids_tensor, val_labels_tensor)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "B9Mtlvm-BRda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "B4RRtrIqE-Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "9lESRRY5jlZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "def epoch_validation(model, classifier, val_loader):\n",
        "    classifier.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)[1]\n",
        "            logits = classifier(outputs)\n",
        "            val_preds.extend(torch.argmax(logits, dim=1).tolist())\n",
        "            val_labels.extend(labels.tolist())\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    precision = precision_score(val_labels, val_preds)\n",
        "    recall = recall_score(val_labels, val_preds)\n",
        "    return val_acc, precision, recall"
      ],
      "metadata": {
        "id": "GuSczK-CyawO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom classifier with a linear layer on top of the pooled output\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "# def epoch_validation(model, classifier, val_loader):\n",
        "\n",
        "#     classifier.eval()\n",
        "#     val_preds = []\n",
        "#     val_labels = []\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in val_loader:\n",
        "#             inputs, labels = inputs.to(device), labels.to(device)\n",
        "#             outputs = model(inputs)[1]\n",
        "#             logits = classifier(outputs)\n",
        "#             val_preds.extend(torch.argmax(logits, dim=1).tolist())\n",
        "#             val_labels.extend(labels.tolist())\n",
        "\n",
        "#     val_acc = accuracy_score(val_labels, val_preds)\n",
        "#     return val_acc\n",
        "\n",
        "\n",
        "# Baseline approach\n",
        "def train_baseline(model, classifier, train_loader, val_loader):\n",
        "    # Freeze the parameters of the BERT model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    classifier.train()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        avg_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)[1]  # Get pooled output from BERT\n",
        "            logits = classifier(outputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss\n",
        "\n",
        "        val_acc, precision, recall = epoch_validation(model, classifier, val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {val_acc}\")\n",
        "        print(\"Validation Precision:\", precision)\n",
        "        print(\"Validation Recall:\", recall)\n",
        "        print(\"Train loss: \", avg_loss.item())\n",
        "        print(\"_______________________\")"
      ],
      "metadata": {
        "id": "MA0ISIOQFkmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "RiapQEC8ncfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Classifier(input_size=768, num_classes=2)\n",
        "classifier = classifier.to(device)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Vnpj6HFyHRNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "print(\"device: \", device)\n",
        "train_baseline(model, classifier, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "whyZSqZBGtAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ZWe8FVe9xpdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention map extraction"
      ],
      "metadata": {
        "id": "jj966EbjkOmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "YvMe3nFFecEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer with output attentions\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
        "\n",
        "# Tokenize input text\n",
        "#input_text = \"There is snowing today\"\n",
        "input_text = \"It was raining yesterday\"\n",
        "input_ids = tokenizer.encode(input_text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass through the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "\n",
        "# Extract attention matrices from the output\n",
        "attention_matrices = outputs.attentions  #Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n"
      ],
      "metadata": {
        "id": "T1Q-NI07HW3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print attention_matrices\n",
        "print(len(attention_matrices))"
      ],
      "metadata": {
        "id": "S0aViiKEH9KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "id": "fhuazlASIO6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(attention_matrices[0]))"
      ],
      "metadata": {
        "id": "c5jMTrC9IH7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/77546636/understanding-output-attentions"
      ],
      "metadata": {
        "id": "ikNSfU_g1MFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_matrices[-1].shape # 12 heads   # lets take last layer attention"
      ],
      "metadata": {
        "id": "Rx37_CCHILuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming you have the attention map stored in the variable attention_map\n",
        "#attention_map = torch.randn(1, 12, 8, 8)  # Example random attention map\n",
        "attention_map = attention_matrices[-1]\n",
        "# Extract attention map for the 5th head\n",
        "attention_map_5th_head = attention_map[:, 2, :, :]  # Extracting the 5th head, index 4 because indexing starts from 0\n",
        "\n",
        "# Convert the attention map tensor to a NumPy array\n",
        "attention_map_5th_head_np = attention_map_5th_head.squeeze().numpy()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.imshow(attention_map_5th_head_np, cmap='hot', interpolation='nearest')\n",
        "plt.title('Attention Map for 5th Head')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Sequence Length')\n",
        "plt.colorbar()  # Add color bar indicating the intensity of attention\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VL4ovJSz2FdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Hello My name is Kamil\"\n",
        "tokens = tokenizer.tokenize(input_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "7e24y57f2lOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming you have the attention maps stored in a tuple named attention_maps\n",
        "\n",
        "\n",
        "# Create a grid of subplots to visualize each head's attention map\n",
        "fig, axes = plt.subplots(3, 4, figsize=(15, 10))  # Assuming 12 heads, arrange in a 3x4 grid\n",
        "fig.suptitle('Attention Maps for All Heads')\n",
        "attention_map = attention_matrices[0]\n",
        "# Plot each attention map as a heatmap\n",
        "for i in range(len(attention_matrices)):\n",
        "    #print(i)\n",
        "    attention = attention_map[:, i, :, :]\n",
        "    #print(attention)\n",
        "    ax = axes[i // 4, i % 4]  # Get the appropriate subplot\n",
        "    ax.set_title(f'Head {i+1}')  # Set title for the subplot\n",
        "    ax.imshow(attention.squeeze().numpy(), cmap='hot', interpolation='nearest')\n",
        "    ax.set_xlabel('Sequence Length')\n",
        "    ax.set_ylabel('Sequence Length')\n",
        "    plt.colorbar(ax.imshow(attention.squeeze().numpy(), cmap='hot', interpolation='nearest'), ax=ax)  # Add color bar indicating the intensity of attention\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "euplyiEf4mkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZG2pNZLMWrTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction"
      ],
      "metadata": {
        "id": "p1xoY-KIKcYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install giotto-tda"
      ],
      "metadata": {
        "id": "luKXpxSgKyT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng(42)  # Create a random number generator\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "from gtda.graphs import GraphGeodesicDistance\n",
        "from gtda.homology import VietorisRipsPersistence, SparseRipsPersistence, FlagserPersistence\n",
        "\n",
        "from igraph import Graph\n",
        "\n",
        "from IPython.display import SVG, display"
      ],
      "metadata": {
        "id": "-UWMB3NBKtOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single weighted adjacency matrix of a FCW graph\n",
        "n_vertices = 10\n",
        "x = rng.random((n_vertices, n_vertices))\n",
        "# Fill the diagonal with zeros (not always necessary, see below)\n",
        "np.fill_diagonal(x, 0)\n",
        "\n",
        "# Create a trivial collection of weighted adjacency matrices, containing x only\n",
        "X = [x]\n",
        "\n",
        "# Instantiate topological transformer\n",
        "VR = VietorisRipsPersistence(metric=\"precomputed\")\n",
        "\n",
        "# Compute persistence diagrams corresponding to each entry (only one here) in X\n",
        "diagrams = VR.fit_transform(X)\n",
        "\n",
        "print(f\"diagrams.shape: {diagrams.shape} ({diagrams.shape[1]} topological features)\")"
      ],
      "metadata": {
        "id": "0aMSpEP3Kgm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diagrams"
      ],
      "metadata": {
        "id": "wj2p1zMaKj-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we make one scatter plot per available homology dimension, and plot births and deaths as x- and y-coordinates of points in 2D, we end up with a 2D representation of diagrams[i], and the reason why it is called a persistence diagram:"
      ],
      "metadata": {
        "id": "pJ-LcEGBOz1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the diagram, each point indicates a topological feature in the data which appears at a certain “birth” scale and remains present all the way up to a later “death” scale. A point’s distance from the diagonal is directly proportional to the difference between the point’s “death” and its “birth”. Hence, this distance visually communicates how “persistent” the associated topological feature is. Topological features are partitioned by dimension using colors: above, features in dimension 0 are red while those in dimension 1 are green. In dimension 0, the diagram describes connectivity structure in the data in a very similar way to linkage clustering: we see three points along the vertical axis, which are in one-to-one correspondence with “merge” events in the sense of hierarchical clustering. In dimension 1, the diagram describes the presence of “independent” one-dimensional holes in the data: as expected, there are only two significant points, corresponding to the two “persistent” circles."
      ],
      "metadata": {
        "id": "6ykKwT4IR9ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtda.plotting import plot_diagram\n",
        "\n",
        "plot_diagram(diagrams[0])"
      ],
      "metadata": {
        "id": "xYTTR5rNOM1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 4, figsize=(15, 10))\n",
        "fig.suptitle('Attention Maps for All Heads')\n",
        "attention_map = attention_matrices[0]\n",
        "\n",
        "for i in range(len(attention_matrices)):\n",
        "\n",
        "    attention = attention_map[:, i, :, :]\n",
        "    X = [attention[0].numpy()]\n",
        "    VR = VietorisRipsPersistence(metric=\"precomputed\")\n",
        "    diagrams = VR.fit_transform(X)\n",
        "    print(\"Head: \", i + 1)\n",
        "    print(f\"diagrams.shape: {diagrams.shape} ({diagrams.shape[1]} topological features)\")\n",
        "    print(\"__________________________________________________\")\n",
        "    ax = axes[i // 4, i % 4]\n",
        "    ax.set_title(f'Head {i+1}')\n",
        "    ax.imshow(attention.squeeze().numpy(), cmap='hot', interpolation='nearest')\n",
        "    ax.set_xlabel('Sequence Length')\n",
        "    ax.set_ylabel('Sequence Length')\n",
        "    plt.colorbar(ax.imshow(attention.squeeze().numpy(), cmap='hot', interpolation='nearest'), ax=ax)  # Add color bar indicating the intensity of attention\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UAmX2t1CokjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_map = attention_matrices[0]\n",
        "attention = attention_map[:, 9, :, :]\n",
        "X = [attention[0].numpy()]\n",
        "VR = VietorisRipsPersistence(metric=\"precomputed\")\n",
        "diagrams = VR.fit_transform(X)\n",
        "print(f\"diagrams.shape: {diagrams.shape} ({diagrams.shape[1]} topological features)\")\n",
        "print(diagrams[0])\n",
        "plot_diagram(diagrams[0])"
      ],
      "metadata": {
        "id": "DYysN8bqo_1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGjuVcPwpBT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• The sum of lengths of bars;\n",
        "\n",
        "• The mean of lengths of bars;\n",
        "\n",
        "• The variance of lengths of bars;\n",
        "\n",
        "• The number of bars with time of birth/death\n",
        "greater/lower than threshold;\n",
        "\n",
        "• The time of birth/death of the longest bar (excluding infinite);\n",
        "\n",
        "• The overall number of bars;\n",
        "\n",
        "• The entropy of the barcode."
      ],
      "metadata": {
        "id": "YpMIYX9Muul1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diagrams"
      ],
      "metadata": {
        "id": "bf7Y_2M8zEnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(diagrams[0])"
      ],
      "metadata": {
        "id": "wkkzPSsVu-DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction pipeline"
      ],
      "metadata": {
        "id": "f1Ajfdquer6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def histogram_entropy(hist):\n",
        "    total = np.sum(hist)\n",
        "    probabilities = hist / total\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Adding a small value to avoid log(0)\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "_U7xutOmTIUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def histogram_entropy(hist):\n",
        "    total = np.sum(hist)\n",
        "    probabilities = hist / total\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Adding a small value to avoid log(0)\n",
        "    return entropy\n",
        "\n",
        "# Example histogram (replace this with your own histogram data)\n",
        "histogram_data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "\n",
        "# Compute entropy\n",
        "entropy = histogram_entropy(histogram_data)\n",
        "print(\"Entropy:\", entropy)\n",
        "print(histogram_entropy(diagrams))"
      ],
      "metadata": {
        "id": "msudjcafwdV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_features(diagrams):\n",
        "    diagrams_list = diagrams[0]\n",
        "    number_of_bars_h0 = 0\n",
        "    number_of_bars_h1 = 0\n",
        "    length_h0 = list()\n",
        "    length_h1 = list()\n",
        "    diag_h0 = [i for i in diagrams[0] if i[2] == 0]\n",
        "    diag_h1 = [i for i in diagrams[0] if i[2] == 1]\n",
        "    entr_h0 = histogram_entropy([i for i in diagrams[0] if i[2] == 0])\n",
        "    entr_h1 = histogram_entropy([i for i in diagrams[0] if i[2] == 1])\n",
        "\n",
        "    for bar in diagrams_list:\n",
        "        birth = bar[0]\n",
        "        death = bar[1]\n",
        "        homology_type = bar[2]\n",
        "        if homology_type == 0. :\n",
        "            #print(\"H0\")\n",
        "            length = death - birth\n",
        "            length_h0.append(length)\n",
        "            number_of_bars_h0 +=1\n",
        "        if homology_type == 1. :\n",
        "            #print(\"H1\")\n",
        "            length = death - birth\n",
        "            length_h1.append(length)\n",
        "            number_of_bars_h1 +=1\n",
        "\n",
        "    # print(length_h0)\n",
        "    # print(length_h1)\n",
        "    # print(number_of_bars_h0, number_of_bars_h1)\n",
        "    time_of_birth_longest_h0 = diag_h0[length_h0.index(max(length_h0))][0]\n",
        "    time_of_birth_longest_h1 = diag_h1[length_h1.index(max(length_h1))][0]\n",
        "    time_of_death_longest_h0 = diag_h0[length_h0.index(max(length_h0))][1]\n",
        "    time_of_death_longest_h1 = diag_h1[length_h1.index(max(length_h1))][1]\n",
        "    #print(time_of_birth_longest_h0 ,time_of_birth_longest_h1 ,time_of_death_longest_h0 ,time_of_death_longest_h1)\n",
        "    sum_of_lenghts_h0 = sum(length_h0)\n",
        "    sum_of_lenghts_h1 = sum(length_h1)\n",
        "    mean_of_lenghts_h0 = np.mean(np.array(length_h0))\n",
        "    mean_of_lenghts_h1 = np.mean(np.array(length_h1))\n",
        "    var_of_lenghts_h0 = np.var(np.array(length_h0))\n",
        "    var_of_lenghts_h1 = np.var(np.array(length_h1))\n",
        "    if number_of_bars_h1 == 0 :\n",
        "        entr_h1= 0\n",
        "        time_of_birth_longest_h1= 0\n",
        "        time_of_death_longest_h1= 0\n",
        "        sum_of_lenghts_h1 = 0\n",
        "        mean_of_lenghts_h1= 0\n",
        "        var_of_lenghts_h1 = 0\n",
        "    if number_of_bars_h0 == 0 :\n",
        "        entr_h0 = 0\n",
        "        time_of_birth_longest_h0= 0\n",
        "        time_of_death_longest_h0= 0\n",
        "        sum_of_lenghts_h0 = 0\n",
        "        mean_of_lenghts_h0= 0\n",
        "        var_of_lenghts_h0 = 0\n",
        "\n",
        "\n",
        "    #print(sum_of_lenghts_h0, sum_of_lenghts_h1, mean_of_lenghts_h0, mean_of_lenghts_h1, var_of_lenghts_h0, var_of_lenghts_h1)\n",
        "    feature_list = [entr_h0, entr_h1, number_of_bars_h0,number_of_bars_h1,time_of_birth_longest_h0,time_of_birth_longest_h1,time_of_death_longest_h0,\n",
        "                    time_of_death_longest_h1,sum_of_lenghts_h0, sum_of_lenghts_h1, mean_of_lenghts_h0,mean_of_lenghts_h1,var_of_lenghts_h0, var_of_lenghts_h1 ]\n",
        "    return {\n",
        "                'length_h0' : length_h0, # not a feature\n",
        "                'length_h1' : length_h1, # not a feature\n",
        "                'entropy_h0': entr_h0,\n",
        "                'entropy_h1': entr_h1,\n",
        "                'number_of_bars_h0' : number_of_bars_h0,\n",
        "                'number_of_bars_h1' : number_of_bars_h1,\n",
        "                'time_of_birth_longest_h0' : time_of_birth_longest_h0,\n",
        "                'time_of_birth_longest_h1' : time_of_birth_longest_h1,\n",
        "                'time_of_death_longest_h0' : time_of_death_longest_h0,\n",
        "                'time_of_death_longest_h1' : time_of_death_longest_h1,\n",
        "                'sum_of_lenghts_h0' : sum_of_lenghts_h0,\n",
        "                'sum_of_lenghts_h1' : sum_of_lenghts_h1,\n",
        "                'mean_of_lenghts_h0' : mean_of_lenghts_h0,\n",
        "                'mean_of_lenghts_h1' : mean_of_lenghts_h1,\n",
        "                'var_of_lenghts_h0' : var_of_lenghts_h0,\n",
        "                'var_of_lenghts_h1' : var_of_lenghts_h1,\n",
        "                'features' : feature_list\n",
        "        }\n"
      ],
      "metadata": {
        "id": "0YxlWKLss_DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_features(diagrams)"
      ],
      "metadata": {
        "id": "N6uNd5eWs_F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = attention_map[:, 4, :, :].squeeze().numpy()\n",
        "X = [attention]\n",
        "VR = VietorisRipsPersistence(metric=\"precomputed\")\n",
        "diagrams = VR.fit_transform(X)\n",
        "print(f\"diagrams.shape: {diagrams.shape} ({diagrams.shape[1]} topological features)\")\n",
        "print(diagrams)\n",
        "compute_features(diagrams)"
      ],
      "metadata": {
        "id": "Pjl_oVFjEhJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "5NSQRA18GOoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "xgHILU56lj3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "# print(\"device:\", device)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
        "# model = model.to(device)\n",
        "# all_features = list()\n",
        "\n",
        "# for data, labels in tqdm(train_loader):\n",
        "#     data, labels = data.to(device), labels.to(device)\n",
        "#     #print(data.shape)\n",
        "\n",
        "#     # print(data[31].numpy())\n",
        "#     # print(data[31].numpy()[10])\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(data)\n",
        "\n",
        "#     attention_matrices = outputs.attentions  #Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
        "#     attention_map = attention_matrices[-1] # from the last layer\n",
        "#     #print(attention_map.shape)\n",
        "#     #print(attention_map[31].shape)\n",
        "\n",
        "#     for i in range(batch_size):\n",
        "#         list_features = list()\n",
        "#         attention_for_sample = attention_map[i]\n",
        "#         for head in range(12):\n",
        "#             #print(attention_for_sample.shape)\n",
        "#             attention = attention_for_sample[head, :, :].cpu().numpy()\n",
        "#             #print(attention.shape)\n",
        "#             X = [attention]\n",
        "#             VR = VietorisRipsPersistence(metric=\"precomputed\")\n",
        "#             diagrams = VR.fit_transform(X)\n",
        "#             #print(f\"diagrams.shape: {diagrams.shape} ({diagrams.shape[1]} topological features)\")\n",
        "#             #print(diagrams)\n",
        "#             features = compute_features(diagrams)['features']\n",
        "#             list_features.append(features)\n",
        "#         all_features.append(np.array(list_features))\n",
        "#     break\n"
      ],
      "metadata": {
        "id": "Km44_lcrGsaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "print(\"device:\", device)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
        "model = model.to(device)\n",
        "all_features = list()\n",
        "c = 0\n",
        "for data, labels in tqdm(train_loader):\n",
        "    data, labels = data.to(device), labels.to(device)\n",
        "    #print(data.shape)\n",
        "\n",
        "    # print(data[31].numpy())\n",
        "    # print(data[31].numpy()[10])\n",
        "    with torch.no_grad():\n",
        "        outputs = model(data)\n",
        "\n",
        "    attention_matrices = outputs.attentions  #Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
        "    attention_map = attention_matrices[-1] # from the last layer\n",
        "    #print(attention_map.shape)\n",
        "    #print(attention_map[31].shape)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        list_features = list()\n",
        "        attention_for_sample = attention_map[i]\n",
        "\n",
        "        #print(attention_for_sample.shape)\n",
        "        attention = attention_for_sample[-1, :, :].cpu().numpy()\n",
        "        #print(attention.shape)\n",
        "        X = [attention]\n",
        "        VR = VietorisRipsPersistence(metric=\"precomputed\")\n",
        "        diagrams = VR.fit_transform(X)\n",
        "        #print(f\"diagrams.shape: {diagrams.shape} ({diagrams.shape[1]} topological features)\")\n",
        "        #print(diagrams)\n",
        "        features = compute_features(diagrams)['features']\n",
        "        #list_features.append(features)\n",
        "        all_features.append(np.array(features))\n",
        "    # c+=1\n",
        "    # if c == 3:\n",
        "    #     break"
      ],
      "metadata": {
        "id": "3T3W4PVUqysZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "file_path = 'features.npy'\n",
        "# Save the array to file\n",
        "np.save(file_path, np.array(all_features))\n"
      ],
      "metadata": {
        "id": "URCGcgP-wLTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to /content/drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "zxf5Av5-wekT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Specify the file path in your Google Drive\n",
        "file_path = '/content/drive/My Drive/features_array.npy'\n",
        "\n",
        "# Save the array to file\n",
        "np.save(file_path, np.array(all_features))\n",
        "\n"
      ],
      "metadata": {
        "id": "-DFBBMDnwfny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features[2].shape"
      ],
      "metadata": {
        "id": "GlnokWFOV4iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features[0]"
      ],
      "metadata": {
        "id": "9nXktuQRqKmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all = np.array(all_features)\n",
        "all.shape"
      ],
      "metadata": {
        "id": "UsI-ijNlV8f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_train)"
      ],
      "metadata": {
        "id": "kfTDBn-10Pwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Assuming X is your feature matrix and y is your target variable\n",
        "# Split the data into training and test sets\n",
        "all[all == np.inf] = 1000\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(all, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train2_scaled = scaler.fit_transform(X_train2)\n",
        "imputer = SimpleImputer(strategy='mean') # Initialize SimpleImputer with strategy='mean'\n",
        "X_train2_imputed = imputer.fit_transform(X_train2_scaled) # Fit the imputer on X_train2 and transform X_train2\n",
        "\n",
        "\n",
        "LogReg = LogisticRegression()\n",
        "LogReg.fit(X_train2_imputed, y_train2)# Train the model on the training data\n",
        "\n",
        "\n",
        "X_test2_scaled = scaler.transform(X_test2)  # Use the same scaler fitted on the training data\n",
        "X_test2_imputed = imputer.transform(X_test2_scaled)\n",
        "\n",
        "y_pred2 = LogReg.predict(X_test2_imputed) # Make predictions on the test data\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test2, y_pred2)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Additional evaluation metrics\n",
        "print(classification_report(y_test2, y_pred2))\n"
      ],
      "metadata": {
        "id": "kOfSGXcGF1vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-zp2EvtJXjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "orpFG2ciF103"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-pMYJEeF13m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSpw4XMNF16s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention-based approach"
      ],
      "metadata": {
        "id": "mvNCiMZBj-oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def val_attention(classifier, model, val_loader):\n",
        "    classifier.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            attention_weights = outputs.attentions\n",
        "            features = derive_features_from_attention(attention_weights)\n",
        "            logits = classifier(features)\n",
        "            val_preds.extend(torch.argmax(logits, dim=1).tolist())\n",
        "            val_labels.extend(labels.tolist())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    return val_acc\n"
      ],
      "metadata": {
        "id": "jjjVPfcqZNZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention-based approach\n",
        "def train_attention(classifier,model,train_loader, val_loader):\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss = 0\n",
        "        classifier.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # Extract attention maps from BERT\n",
        "            outputs = model(inputs)\n",
        "            attention_weights = outputs.attentions  # Assuming model returns attention weights\n",
        "            # Use attention weights to derive features (implementation needed)\n",
        "            features = derive_features_from_attention(attention_weights)\n",
        "            logits = classifier(features)\n",
        "            loss = criterion(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss\n",
        "\n",
        "        print(\"Train loss: \", avg_loss)\n",
        "        val_acc = epoch_validation(model, classifier, val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {val_acc}\")\n",
        "        print(\"Train loss: \", avg_loss)\n",
        "        print(\"_______________________\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "07dNxvnY6l-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classifier\n",
        "classifier = Classifier(input_size=768, num_classes=2)\n",
        "classifier = classifier.to(device)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "VdioNDiCaTFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have loaded and preprocessed your data into PyTorch DataLoader objects train_loader and val_loader\n",
        "num_epochs = 5\n",
        "train_attention(train_loader, val_loader)"
      ],
      "metadata": {
        "id": "-ACqzgv5kDIS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}